{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5ueTTt9D46GzOzqB7Q2Xm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toche7/AI_ITM/blob/main/Lab11_Exampleofpythainlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyWTAe3UILNe"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import word_tokenize, pos_tag\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "from pythainlp.util import normalize\n",
        "\n",
        "# Example Thai text\n",
        "text = \"สวัสดีครับ วันนี้อากาศดีมากกๆ\"\n",
        "\n",
        "# Cleaning: Normalize the text to ensure consistency\n",
        "text_normalized = normalize(text)\n",
        "text_normalized"
      ],
      "metadata": {
        "id": "aHgWWSE8WW80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and word segmentation: Split text into words (default engine is 'newmm')\n",
        "tokens = word_tokenize(text_normalized)\n",
        "tokens"
      ],
      "metadata": {
        "id": "0IFT3CONWig7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "stopwords = thai_stopwords()\n",
        "tokens_without_stopwords = [word for word in tokens if word not in stopwords]\n",
        "tokens_without_stopwords"
      ],
      "metadata": {
        "id": "IH-QcoqPWvS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part-of-Speech Tagging\n",
        "pos_tags = pos_tag(tokens_without_stopwords)\n",
        "pos_tags"
      ],
      "metadata": {
        "id": "yWzaSZy_IXzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet performs the following steps:\n",
        "* Normalizes the text to a consistent form.\n",
        "* Tokenizes the text into words.  Tokenization in Thai is mainly about word segmentation due to the absence of space between words.\n",
        "* Removes stopwords, which are words that usually carry no significant meaning by themselves.\n",
        "* Tags each word with its part-of-speech.\n",
        "\n",
        "Unfortunately, as mentioned earlier, stemming is not a typical process in Thai text processing because words do not have the same kind of morphological variations as in languages like English. Therefore, the process usually stops at word segmentation and part-of-speech tagging.\n"
      ],
      "metadata": {
        "id": "xl30ovD-J98O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NPKH2XHQKfQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}